{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Parking Ticket Prediction Using Apache Spark ML\n",
    "\n",
    "Abstract: _Poor traffic and parking management in large metropolitan cities negatively impacts our living conditions. Additionally, it can be a large source of revenue for these cities, amounting to half a billion dollars yearly in New York City. In this work, we analyze parking ticket data for the period 2015-2020 in NYC, aggregating it with weather, time, holiday and location features with the goal of predicting the number of tickets given hourly at a particular location. Due to the large volume of the original data and the existence of several data sources with different formats and frequencies, we constructed our pipeline in a distributed way using PySpark.\n",
    "    Our goal is not only to predict the number of parking tickets, but also to gain insight into the data and models by extracting feature importance, which can be valuable in the context of urban planning and traffic management._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark session libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "## Required for parsing the file as csv\n",
    "import csv\n",
    "from io import StringIO\n",
    "from itertools import islice, repeat\n",
    "\n",
    "## For preprocessing\n",
    "from re import search, split, sub, compile as comp\n",
    "import numpy as np\n",
    "from statistics import median\n",
    "\n",
    "## For Plots & other\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib import cm, colors\n",
    "import pandas as pd\n",
    "from numpy import allclose\n",
    "import warnings\n",
    "import requests, json, time, random\n",
    "from collections import Counter\n",
    "\n",
    "## for models\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import types\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import KMeansModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports `folium`, which is a simple and interactive Python library which makes working with geographical data easy. To run the cell, first do: `conda install -c conda-forge folium`. Note: this is not necessary for model training, only visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RULL THIS CELL\n",
    "import folium \n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Spark context and session\n",
    "\n",
    "Note: [*\\] runs with the maximum available number of threads --- we used 12 logical threads on a 6 core PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8\n",
      "local[*,4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. \n",
      "Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. \n",
      "To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. \n",
      "To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*,4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SDDM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*,4] appName=SDDM>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"SDDM\", master='local[*,4]')\n",
    "sc.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")\n",
    "print(sc.pythonVer)\n",
    "print (sc.master)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .appName('SDDM2') \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32g\")\\\n",
    "    .config(\"spark.executor.memory\", \"64g\")\\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCSV(csvRow) :\n",
    "    '''Parses a row into a list of elements'''\n",
    "    data = StringIO(csvRow)\n",
    "    dataReader = csv.reader(data, lineterminator = '')\n",
    "    return(next(dataReader))\n",
    "\n",
    "def readFileAsCSV(session, filepath):\n",
    "    '''Reads a files as text file and then parses each row and returns a list of list: \n",
    "        [[Row]\n",
    "         [Row]\n",
    "         [Row]]\n",
    "     '''\n",
    "    try:\n",
    "        data = session.textFile(name = str(filepath))\n",
    "        data = data.map(parseCSV)\n",
    "    except:\n",
    "        print('Failed to read the file!')\n",
    "        data = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticket data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tickTime(x, ind):\n",
    "    '''Extracts the month and year from the issue date'''\n",
    "    try:\n",
    "        m = int(x[ind][0:2])\n",
    "        y = int(x[ind][6:10])\n",
    "    except:\n",
    "        m = '0'\n",
    "        y = '0'    \n",
    "    x.append(str(m))\n",
    "    x.append(str(y))\n",
    "    return x\n",
    "\n",
    "def street_preprocess(x, ind):\n",
    "    '''Preprocess the street names'''\n",
    "    s = x[ind]\n",
    "    s = s.replace('AVENUE','AVE').replace('STREET','ST').replace('BLVD','BL')\n",
    "    s = s.replace('\\sEAST\\s',' E ').replace('\\sWEST\\s',' W ').replace('\\sNORTH\\s',' N ').replace('\\sSOUTH\\s',' S ')\n",
    "    s = s.replace('\\sROAD\\s',' RD ').replace('\\sEXPY\\s','EXWY').replace('\\sPARKWY\\s','PKWY').replace('\\sISLAND\\s','ISL')\n",
    "    s = s.replace('\\sFIRST\\s','1').replace('\\sSECOND\\s','2').replace('\\sTHRID\\s','3')\n",
    "    s = s.replace('\\sFOURTH\\s','4').replace('\\sFIVETH\\s','5').replace('\\sSIXTH\\s','6')\n",
    "    s = s.replace('\\sSEVENTH\\s','7').replace('\\sEIGHTH\\s','8').replace('\\sNINETH\\s','9').replace('\\sTENTH\\s','10')\n",
    "    s = s.split()\n",
    "    result = [x if not search(r'\\d', x) else sub('[^0-9]','', x) for x in s]\n",
    "    result = ' '.join(result)\n",
    "    x[ind] = result.lower()\n",
    "    return x\n",
    "\n",
    "def rState(x, ind):\n",
    "    if x[ind] == 'NY':\n",
    "        x.append('0')\n",
    "    else:\n",
    "        x.append('1')\n",
    "    return x\n",
    "\n",
    "def violationType(x, ind):\n",
    "    mydict = {\"Misc\":[35,41,90,91,94],\n",
    "                        \"No Parking\":[20,21,23,24,27],\n",
    "                        \"No Standing\":[3,4,5,6,8,10,11,12,13,14,15,16,17,18,19,22,25,26,30,31,40,44,54,57,58,63,64,77,78,81,89,92],\n",
    "                        \"Permit/Doc Issue\":[1,2,29,70,71,72,73,76,80,83,87,88,93,97],\n",
    "                        \"Plate Issues\":[74,75,82],\n",
    "                        \"Obstructing Path\":[7,9,36,45,46,47,48,49,50,51,52,53,55,56,59,60,61,62,66,67,68,79,84,96,98],\n",
    "                        \"Overtime\":[28,32,33,34,37,38,39,42,43,65,69,85,86]\n",
    "                        }\n",
    "    label = ''\n",
    "    try:\n",
    "        for key, value in mydict.items():\n",
    "             for y in value:\n",
    "                    if y == int(x[ind]):\n",
    "                        label = key\n",
    "    except: \n",
    "        label = ''\n",
    "    newLabs = {0:'',\n",
    "               1:\"Misc\",\n",
    "               2:\"No Parking\",\n",
    "               3:\"No Standing\",\n",
    "               4:\"Permit/Doc Issue\",\n",
    "               5:\"Plate Issues\",\n",
    "               6:\"Obstructing Path\",\n",
    "               7:\"Overtime\"}\n",
    "    x.append(str(list(newLabs.keys())[list(newLabs.values()).index(label)]))\n",
    "    x[ind] = label\n",
    "    return x\n",
    "    \n",
    "def sH(x, ind):\n",
    "    ''' Extracts the street number and house number from House number column\n",
    "        Some house numbers are: 123-34, 45-56 and some are 34, 45 etc\n",
    "    '''\n",
    "    house_num = x[ind]\n",
    "    try:\n",
    "        if house_num == '':\n",
    "            s = '0'\n",
    "            h = '0'\n",
    "        else:\n",
    "            cond = '-' in house_num\n",
    "            if cond:\n",
    "                s, h = house_num.split('-')\n",
    "            else:\n",
    "                s = int(house_num)\n",
    "                h = '0'\n",
    "    except:\n",
    "        s = '0'\n",
    "        h = '0'\n",
    "    x.append(str(s))\n",
    "    x.append(str(h))\n",
    "    return x\n",
    "    \n",
    "def preprocessedCSV(session, filepath):\n",
    "    '''\n",
    "    Reads the csv files, and then converts Issue date to date and month\n",
    "    '''\n",
    "    data = readFileAsCSV(session, filepath)\n",
    "    header = data.take(1)[0]\n",
    "    data = data.map(lambda x: [x[0], x[2], x[3], x[4], x[5], x[6], x[7], x[19], x[23], x[24]])\n",
    "    header = data.take(1)[0]\n",
    "    #print(header)\n",
    "    ## Extracting the month and year\n",
    "    data = data.map(lambda x: tickTime(x, header.index('Issue Date')))\n",
    "    header.append('Issue Month')\n",
    "    header.append('Issue Year')\n",
    "    \n",
    "    ## Removing the header line\n",
    "    data = data.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "    ## Preprocessing the street names\n",
    "    data = data.map(lambda x: street_preprocess(x, header.index('Street Name')))    \n",
    "    ## Extracts the street and house number\n",
    "    data = data.map(lambda x: sH(x, header.index('House Number')))\n",
    "    header.append('Street')\n",
    "    header.append('House')\n",
    "    data = data.map(lambda x: rState(x, header.index('Registration State')))\n",
    "    header.append('RState')\n",
    "    data = data.map(lambda x: violationType(x, header.index('Violation Code')))\n",
    "    header.append('VType')\n",
    "    return data, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinate file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streetHouse(x):\n",
    "    '''\n",
    "        Extracts the street and house number ranges for a street name and coordinate\n",
    "    '''\n",
    "    house_num = x[0]\n",
    "    try:\n",
    "        if house_num == '':\n",
    "            l_s_min = '0'\n",
    "            l_s_max = '0'\n",
    "            r_s_min = '0'\n",
    "            r_s_max = '0'\n",
    "\n",
    "            h_l_min = '0'\n",
    "            h_l_max = '0'\n",
    "            h_r_min = '0'\n",
    "            h_r_max = '0'\n",
    "        else:\n",
    "            cond = '-' in house_num\n",
    "            if cond:\n",
    "                l_s_min, h_l_min = x[0].split('-')\n",
    "                l_s_max, h_l_max = x[1].split('-')\n",
    "                r_s_min, h_r_min = x[4].split('-')\n",
    "                r_s_max, h_r_max = x[5].split('-')\n",
    "            else:\n",
    "                l_s_min = int(x[0])\n",
    "                l_s_max = int(x[1])\n",
    "                r_s_min = int(x[4])\n",
    "                r_s_max = int(x[5])\n",
    "                h_l_min = '0'\n",
    "                h_l_max = '0'\n",
    "                h_r_min = '0'\n",
    "                h_r_max = '0'\n",
    "\n",
    "    except:\n",
    "        l_s_min = '0'\n",
    "        l_s_max = '0'\n",
    "        r_s_min = '0'\n",
    "        r_s_max = '0'\n",
    "        h_l_min = '0'\n",
    "        h_l_max = '0'\n",
    "        h_r_min = '0'\n",
    "        h_r_max = '0'\n",
    "        \n",
    "    x.extend([l_s_min, l_s_max, r_s_min, r_s_max, h_l_min, h_l_max, h_r_min, h_r_max])\n",
    "    return x\n",
    "\n",
    "def geoms(x, ind):\n",
    "    '''Extracting one single latitute and longitude values from the geometry '''\n",
    "    coords = x[ind]\n",
    "    try: \n",
    "        coords = coords.replace('MULTILINESTRING ', '').replace('(','').replace(')', '').split(', ')\n",
    "        coords = [i.split(' ') for i in coords]\n",
    "        coords = [[float(j), float(k)] for j,k in coords]\n",
    "        lon = str(median([j for j,k in coords ]))\n",
    "        lat = str(median([k for j,k in coords ]))\n",
    "    except:\n",
    "        lon = 'NA'\n",
    "        lat = 'NA'\n",
    "    x.append(lon)\n",
    "    x.append(lat)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def createCoordsFiles(session, filepath):\n",
    "    '''Reading the centerline data set and returns a preprocessed RDD'''\n",
    "    coords = readFileAsCSV(session, filepath) \n",
    "    ## Removing the first line\n",
    "    coords = coords.map(lambda x: [x[0],x[1],x[28],x[3],x[4],x[5]])\n",
    "    ## Getting the header\n",
    "    coords_header = coords.take(1)[0]\n",
    "    coords = coords.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "    ## Preprocessing the street names\n",
    "    coords = coords.map(lambda x: street_preprocess(x, coords_header.index('FULL_STREE'))) \n",
    "    ## Finding the street numbers and house number limits\n",
    "    coords = coords.map(streetHouse)\n",
    "    coords = coords.map(lambda x: geoms(x, coords_header.index('the_geom')))\n",
    "    ## Extracting the required columns\n",
    "    #coords = coords.map(lambda x: [x[2],x[6],x[7],x[8],x[9], x[10], x[11]])\n",
    "    coords_header.extend(['L_S_min', 'L_S_max', 'R_S_min', 'R_S_max', 'L_H_min', 'L_H_max', 'R_H_min', 'R_H_max', 'lon', 'lat'] )\n",
    "    return coords, coords_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match the ticket to their respectve coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchstreet(t, c):\n",
    "    '''\n",
    "    Based on the street and house number in tickets dataframe finds the coordintae value in centerline dataframe\n",
    "    and merges the coordinate value to it.\n",
    "    '''\n",
    "    ## Making the columns integers for comparision\n",
    "    t =  t.withColumn(\"Issue_Month\", t[\"Issue_Month\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Issue_Year\", t[\"Issue_Year\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Street\", t[\"Street\"].cast('integer'))\n",
    "    t =  t.withColumn(\"House\", t[\"House\"].cast('integer'))\n",
    "    t =  t.withColumn(\"RState\", t[\"RState\"].cast('integer'))\n",
    "    t =  t.withColumn(\"VType\", t[\"VType\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Ids\", monotonically_increasing_id())\n",
    "    t =  t.withColumn(\"Ids\", t[\"Ids\"].cast('string'))\n",
    "    \n",
    "    c = c.select('FULL_STREE', c.L_S_min.cast('integer'),c.L_S_max.cast('integer'),\\\n",
    "                 c.R_S_min.cast('integer'),c.R_S_max.cast('integer'),\\\n",
    "                 c.L_H_min.cast('integer'),c.L_H_max.cast('integer'),\\\n",
    "                 c.R_H_min.cast('integer'),c.R_H_max.cast('integer'),\\\n",
    "                 c.lon.cast('float'),c.lat.cast('float'))\n",
    "    ## performs inner join on the tickets. \n",
    "    merged = t.join(c, [t.Street_Name == c.FULL_STREE,\\\n",
    "                        (t.Street>=c.L_S_min)  | (t.Street>=c.R_S_min),\\\n",
    "                        (t.Street<=c.L_S_max)  | (t.Street<=c.R_S_max),\\\n",
    "                        (t.House >=c.L_H_min)  | (t.House >=c.R_H_min),\\\n",
    "                        (t.House <= c.L_H_max) | (t.House <= c.R_H_max)],'inner').select('Ids', 'Summons_Number', 'Registration_State', 'Plate_Type', 'Issue_Date', 'Violation_Code', 'Vehicle_Body_Type', 'Vehicle_Make', 'House_Number', 'Street_Name', 'Issue_Month', 'Issue_Year', 'Violation_Time', 'Street', 'House', 'RState', 'VType', 'lon', 'lat')\n",
    "    \n",
    "    return merged.dropDuplicates(subset=['Ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(sc, filepath, filename, year):\n",
    "    '''Reads each file in a loop and returns a list of RDDs'''\n",
    "    tickets = []\n",
    "    for yr in year:\n",
    "        filelocation = str(filepath)+str(filename)+str(yr)+\".csv\"\n",
    "        print(filelocation)\n",
    "        parking_data, header = preprocessedCSV(sc, filelocation)\n",
    "        tickets.append(parking_data)\n",
    "    return tickets, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(data, Val, Key):\n",
    "    pairs = data.map(lambda x: (x[Key], x[Val]))\n",
    "    return pairs.groupByKey().collect()\n",
    "\n",
    "def group_data_toList(y, groupby):\n",
    "    '''\n",
    "    Input:\n",
    "        - y: Grouped pyspark data returned from group_data function\n",
    "        - groupby: Column name to remove an extra element\n",
    "    '''\n",
    "    lab = list(map(lambda x:x[0], y))\n",
    "    val = list(map(lambda x:len(x[1]), y))\n",
    "    try:\n",
    "        kick = lab.index(groupby)\n",
    "        lab.pop(kick)\n",
    "        val.pop(kick)\n",
    "    except:\n",
    "        0\n",
    "    if groupby=='Issue Month' or groupby == 'Violation Code':\n",
    "        lab = list(map(lambda x: int(x), lab))\n",
    "    return [lab,val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ax, header, data, year, groupby, pt, plot_data):\n",
    "    '''\n",
    "    Input:\n",
    "        - data: PySpark parsed CSV\n",
    "        - groupby: Column name for grouping the data \n",
    "        - pt: Plot type\n",
    "        - plot data: Variables required for bar plot\n",
    "        \n",
    "    Output: \n",
    "        - Plot\n",
    "    '''\n",
    "    if pt=='bar':\n",
    "        col, val, axis_labels, legend_labels = plot_data\n",
    "        \n",
    "        cat_index = header.index(col)\n",
    "        \n",
    "        cat1 = data.filter(lambda x: x[cat_index]==val)\n",
    "        cat2 = data.filter(lambda x: x[cat_index]!=val)                   \n",
    "\n",
    "        groupby_index = header.index(groupby)\n",
    "        count_column = header.index('Summons Number')\n",
    "        \n",
    "        cat1 = group_data_toList(group_data(cat1 , Val=count_column, Key=groupby_index), groupby)\n",
    "        cat2 = group_data_toList(group_data(cat2 , Val=count_column, Key=groupby_index), groupby)\n",
    "            \n",
    "        ## plotting the graph\n",
    "        ax.bar(cat1[0], cat1[1], width = 0.5, label=legend_labels[0]) \n",
    "        ax.bar(cat2[0], cat2[1], width = 0.5, label=legend_labels[1])\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.set_xlabel(groupby, fontsize=18)\n",
    "        ax.set_ylabel('Number of tickets', fontsize=18)\n",
    "        ax.set_title('Parking tickets for the year '+str(year-1), fontsize=22)\n",
    "    if pt == 'pie':\n",
    "        #try:\n",
    "            groupby_index = header.index(groupby)\n",
    "            count_column = header.index('Summons Number')\n",
    "            \n",
    "            label, data = group_data_toList(group_data(data , Val=count_column, Key=groupby_index), groupby)\n",
    "            title = 'Parking ticket '+str(groupby)+' for the year '+str(year-1)\n",
    "            \n",
    "            if groupby == 'Violation Code': \n",
    "                ## Defining the violation code merges as a dictionary\n",
    "                labs = {\"Misc\":[35,41,90,91,94],\n",
    "                        \"No Parking\":[20,21,23,24,27],\n",
    "                        \"No Standing\":[3,4,5,6,8,10,11,12,13,14,15,16,17,18,19,22,25,26,30,31,40,44,54,57,58,63,64,77,78,81,89,92],\n",
    "                        \"Permit/Doc Issue\":[1,2,29,70,71,72,73,76,80,83,87,88,93,97],\n",
    "                        \"Plate Issues\":[74,75,82],\n",
    "                        \"Obstructing Path\":[7,9,36,45,46,47,48,49,50,51,52,53,55,56,59,60,61,62,66,67,68,79,84,96,98],\n",
    "                        \"Overtime\":[28,32,33,34,37,38,39,42,43,65,69,85,86]\n",
    "                        }\n",
    "                ## Count based on the grouping\n",
    "            \n",
    "                temp = defaultdict(list)\n",
    "                for i in range(len(label)):\n",
    "                    for key, val in labs.items():\n",
    "                        if label[i] in val:\n",
    "                            if temp[key] == []:\n",
    "                                temp[key] = 0\n",
    "                            else:\n",
    "                                temp[key] = temp[key]+data[i]\n",
    "\n",
    "                ## Ordering data based on the dictionary \n",
    "                label, data  = list(), list()\n",
    "                for key in labs.keys():\n",
    "                    label.append(key)\n",
    "                    data.append(temp[key])    \n",
    "\n",
    "            ## Defining color for each category\n",
    "            temp = defaultdict(list)\n",
    "            for l,c in zip(labs,cm.tab20(range(len(labs)))):\n",
    "                temp[l]=c\n",
    "\n",
    "            centre_circle = plt.Circle((0,0),0.85,fc='white') ## radius to make it like a donut\n",
    "            explode = np.full(len(label), 0.04) ## Gaps between the categories\n",
    "\n",
    "            pat = ax.pie(list(map(lambda x: x*100/sum(data), data)), labels=label, textprops={'fontsize': 20}, autopct='%1.1f%%', startangle=90, pctdistance=0.6, explode = explode)\n",
    "            if groupby == 'Violation Code':\n",
    "                for pie_wedge in pat[0]:\n",
    "                    pie_wedge.set_edgecolor('white')\n",
    "                    pie_wedge.set_facecolor(temp[pie_wedge.get_label()]) # Assigning color code for each catergory\n",
    "\n",
    "            ax.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "            ax.set_title(title, fontsize =22, pad=20)\n",
    "            plt.gcf().gca().add_artist(centre_circle)\n",
    "        #except:\n",
    "        #    print('Failed to plot!')        \n",
    "    return ax \n",
    "\n",
    "def EDA(header, tickets, year, groupby, pt, plotdim, plot_data):\n",
    "    fig = plt.figure(figsize=(30, 25))\n",
    "    axs=plt.GridSpec(plotdim[0], plotdim[1], hspace=0.15, wspace=0.1)\n",
    "    for i in range(len(year)):\n",
    "        plot(fig.add_subplot(axs[i]), header, tickets[i], year[i], groupby, pt, plot_data)\n",
    "    plt.savefig('EDA_'+str(groupby)+'_'+str(pt)+'.png',  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching the ticket with their coordinate values\n",
    "This is done using the join method.\n",
    "First we convert `all_tickets` into a dataframe, then we read the Centerline dataset as a dataframe `coord_df` and then using the `matchStreet` function we match the coordinates based on the left and right range of street numbers and house numbers based on the street names. Then we select the data from the years of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ticket_files(year, filepath, filename, centerline):\n",
    "    \n",
    "    ## Returns a list of RDDs which have data for each year\n",
    "    \n",
    "    tickets, header = getData(sc, filepath, filename, year)\n",
    "    \n",
    "    ## Reading the Centerline data set\n",
    "    \n",
    "    coords, c_header = createCoordsFiles(sc, centerline)\n",
    "    coord_df = ss.createDataFrame(coords, schema=c_header)\n",
    "    \n",
    "    tickets_with_coords = []\n",
    "    \n",
    "    for i in range(len(tickets)):\n",
    "        \n",
    "        ## Converting the RDD list to a pyspark dataframe\n",
    "        \n",
    "        tickets_with_coords.append(matchstreet(ss.createDataFrame(tickets[i], schema=[x.replace(' ', '_') for x in header]), coord_df))\n",
    "        \n",
    "        ## Keeping the dataset which is from 2015 to 2020\n",
    "        \n",
    "        tickets_with_coords[i] = tickets_with_coords[i].filter(F.col('Issue_Year').isin(year))  \n",
    "        \n",
    "    return header, tickets, tickets_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\n",
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2016.csv\n",
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\n",
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2018.csv\n",
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2019.csv\n",
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2020.csv\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# reading files\n",
    "\n",
    "# download data from: https://www.kaggle.com/new-york-city/nyc-parking-tickets\n",
    "filepath = 'nyc-parking-tickets/'\n",
    "filename = 'Parking_Violations_Issued_-_Fiscal_Year_'\n",
    "centerline = 'Centerline.csv'\n",
    "year = list(range(2015, 2021))\n",
    "\n",
    "h, tickets, tc_df = read_ticket_files(year, filepath, filename, centerline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: just informative at this stage, not necessary [DO NOT RUN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Bar plot representing the distribution of tickets over the years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Bar plot\n",
    "\n",
    "category='Registration State'\n",
    "value='NY' \n",
    "axis_labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']  \n",
    "legend_labels= ['New York', 'Other Cities']\n",
    "plot_data = [category, value, axis_labels, legend_labels]\n",
    "\n",
    "groupby = 'Issue Month'\n",
    "EDA(h, tickets, year, groupby, 'bar', [int(len(year)/2),2], plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Pie chart showing the distribution of tickets according to violation code over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Pie chart\n",
    "\n",
    "groupby = 'Violation Code'\n",
    "plot_data = []\n",
    "EDA(h, tickets, year, groupby, 'pie', [int(len(year)/2),2], plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"lon\"], outputCol=\"features\")\n",
    "\n",
    "for i in range(len(year)):\n",
    "    tc_df[i] = vecAssembler.transform(tc_df[i]) #putting the lattitude and longitude together for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# defining kmeans model\n",
    "\n",
    "# defining parameters\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 796\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "kmeans = KMeans(k=NUMBER_OF_CLUSTERS, seed=RANDOM_SEED)  # clusters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL (the model can be loaded from file directly)\n",
    "# training kmeans model\n",
    "\n",
    "model = kmeans.fit(tc_df[0].select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cluster model from file\n",
    "\n",
    "model = KMeansModel.load('nyc_parking_tickets_final/kmodel')\n",
    "\n",
    "for i in range(len(year)):\n",
    "    tc_df[i] = model.transform(tc_df[i]) # transforming every year adds cluster number to a new column added in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting cluster centers \n",
    "\n",
    "ctr = []\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "for center in centers:\n",
    "    ctr.append(center) #extracting centers is easy since while fitting and tansforming the model already saves this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40.84229289, -73.90501314])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr[0] # just for check, the first cluster center "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL\n",
    "# visualization only\n",
    "\n",
    "mc = MarkerCluster()\n",
    "m = folium.Map(\n",
    "    location=[40.767937,-73.982155], # these are center cordinates of new york\n",
    "    zoom_start=12,)\n",
    "\n",
    "\n",
    "for i in range(len(ctr)):\n",
    "    mc.add_child(folium.Marker(location=[ ctr[i][0],ctr[i][1] ])) # adding cluster centers\n",
    "\n",
    "m.add_child(mc)\n",
    "m.save('F:/marker_cluster_example_file.html') #visualising the cluster centers just to ensure their distribution is even "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is the process of extracting amenities. For this process libraries like `Requests` (which handles different API requests) can be used. The actual request has to be sent in the format which is accepted on the actual website. Hence we opted to send it in the format of multiple strings added toghether. The time taken for this process is dependent on traffic on the API server. The usual time taken is 1-4 hours, as a result of the fact that a delay is neccessary after every few requests. This prevents the user from getting kicked out from the API server, since the server has a limit set for each user. To avoid performing the process multiple times, the file provided in the zip package can be used instead of sending requests again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL (use the provided file instead)\n",
    "\n",
    "amenity_names = []\n",
    "amenity_count = []\n",
    "\n",
    "Amenity_per_location =pd.DataFrame()\n",
    "Amenity_per_location_2=pd.DataFrame()\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "for i in range(len(ctr)):\n",
    "    overpass_query = '[out:json];' +'(' + \\\n",
    "    '// query part for: “aminity=*”' +'\\n'+\\\n",
    "    'node[\"amenity\"](around:1000,' + str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    'way[\"amenity\"](around:1000,' +  str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    'relation[\"amenity\"](around:1000,' +  str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    ');' + '\\n'+\\\n",
    "    '// print results'+'\\n'+\\\n",
    "    'out;'+'\\n'+\\\n",
    "    '>;'+'out count;'\n",
    "     \n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    try:\n",
    "        data  = response.json()\n",
    "    \n",
    "    except (requests.exceptions.ConnectionError, json.decoder.JSONDecodeError):\n",
    "        time.sleep(2**1 + random.random()*0.01) #exponential backoff\n",
    "\n",
    "\n",
    "    typeamenity =[]\n",
    "    for l in range(len(data['elements'])):\n",
    "        try:\n",
    "            ind = list(data['elements'][l].keys()).index('tags')\n",
    "        \n",
    "            typeamenity.append(data['elements'][l]['tags']['amenity'])\n",
    "        except:\n",
    "            ind = False\n",
    "\n",
    "    amenity_names.append(list(Counter(typeamenity).keys()))\n",
    "    amenity_count.append(list(Counter(typeamenity).values()))\n",
    "    dictionary = dict(zip(amenity_names[i], amenity_count[i]))\n",
    "    Amenity_per_location =  pd.DataFrame.from_dict(dictionary, orient='index')\n",
    "    Amenity_per_location_2 = pd.concat([Amenity_per_location, Amenity_per_location_2], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL (use the provided file instead)\n",
    "\n",
    "Amenity_per_location_2 = Amenity_per_location_2.T\n",
    "Amenity_per_location_2['prediction'] =np.arange(0,len(Amenity_per_location_2))\n",
    "Amenity_per_location_2.to_csv('F:/Amenity_per_location_2_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 896 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Amenity_per_location_2 = ss.read.csv(\"Amenity_per_location.csv\", header=True, sep=\",\");\n",
    "\n",
    "for i in range(len(year)):\n",
    "    tc_df[i] = Amenity_per_location_2.join(tc_df[i], on=['prediction'], how='right_outer')\n",
    "    tc_df[i] = tc_df[i].withColumn(\"Violation_Time\", F.concat(F.col(\"Violation_Time\"), F.lit(\"M\")))\n",
    "    tc_df[i] = tc_df[i].withColumn(\"Violation_Time\", F.to_timestamp(tc_df[i].Violation_Time, 'KKmmaa'))\n",
    "    tc_df[i] = tc_df[i].withColumn(\"Violation_Time\", F.date_format('Violation_Time', 'HH'))\n",
    "    tc_df[i] = tc_df[i].filter(tc_df[i].Violation_Time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelocation_weather = 'merged_weather_holidays_fixedMissingValues.csv'\n",
    "weather = ss.read.csv(filelocation_weather, header=True)\n",
    "weather = weather.withColumn(\"time\", F.to_timestamp(weather.time, 'HH:mm:ss'))\n",
    "weather = weather.withColumn(\"date\", F.to_timestamp(weather.date, 'yyyy-MM-dd'))\n",
    "weather = weather.withColumn(\"time\", F.date_format('time', 'HH'))\n",
    "weather = weather.withColumn(\"date\", F.date_format('date', 'MM/dd/yyyy'))\n",
    "weather = weather.withColumnRenamed(\"time\", \"Violation_Time\")\n",
    "weather = weather.withColumnRenamed(\"date\", \"Issue_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the API returns 150 amenities, we dropped some which we assumed would have the least correlation to parking tickets\n",
    "# future work should consider adding these as well\n",
    "\n",
    "amenities_to_drop = ['amenity|ice_cream', \n",
    "'animal_boarding', \n",
    "'animal_shelter', \n",
    "'art_centre', \n",
    "'arts_centre', \n",
    "'bicycle_parking',\n",
    "'bicycle_rental', \n",
    "'bicycle_repair_station', \n",
    "'biergarten', \n",
    "'boat_rental', \n",
    "'boat_storage',\n",
    "'car_rental', \n",
    "'car_service', \n",
    "'car_sharing',\n",
    " 'car_wash',\n",
    " 'charging_station',\n",
    " 'clock',\n",
    " 'community_centre',\n",
    " 'compressed_air',\n",
    " 'concert_hall',\n",
    " 'cooking_school',\n",
    " 'courthouse',\n",
    " 'coworking_space',\n",
    " 'dancing_school',\n",
    " 'dentist',\n",
    " 'disused',\n",
    " 'dojo',\n",
    " 'drinking_water',\n",
    " 'driving_school',\n",
    " 'ferry_terminal',\n",
    " 'fire_station',\n",
    " 'food_court',\n",
    " 'fortune_teller',\n",
    " 'fountain',\n",
    " 'fuel',\n",
    " 'graphic_design',\n",
    " 'grave_yard',\n",
    " 'ice_cream',\n",
    " 'internet_cafe',\n",
    " 'karaoke_box',\n",
    " 'language_school',\n",
    " 'library',\n",
    " 'loading_dock',\n",
    " 'meditation_centre',\n",
    " 'monastery',\n",
    " 'motorcycle_parking',\n",
    " 'museum',\n",
    " 'music_school',\n",
    " 'music_venue',\n",
    " 'nail salon',\n",
    " 'nail_salon',\n",
    " 'nursing_home',\n",
    " 'outdoor_seating',\n",
    " 'parking',\n",
    " 'parking_entrance',\n",
    " 'parking_space',\n",
    " 'payment_centre',\n",
    " 'payment_terminal', \n",
    "'picnic_table',\n",
    " 'police',\n",
    " 'post_box',\n",
    " 'post_depot',\n",
    " 'prep_school',\n",
    " 'prison',\n",
    " 'public_bath',\n",
    " 'public_bookcase',\n",
    " 'public_building',\n",
    " 'radio station',\n",
    " 'ranger_station',\n",
    " 'recycling',\n",
    " 'rescue_station',\n",
    " 'research_institute',\n",
    " 'salon',\n",
    " 'self_storage',\n",
    " 'shelter',\n",
    " 'shoe_repair',\n",
    " 'smoking_area',\n",
    " 'social_centre',\n",
    " 'social_facility',\n",
    " 'spa',\n",
    " 'stock_exchange',\n",
    " 'stripclub',\n",
    " 'studio', 'swimming_pool',\n",
    " 'swingerclub',\n",
    " 'taxi',\n",
    " 'telephone',\n",
    " 'theatre',\n",
    " 'toilets',\n",
    " 'tourism',\n",
    " 'townhall',\n",
    " 'training',\n",
    " 'university',\n",
    " 'urgent_care',\n",
    " 'vehicle_inspection',\n",
    " 'vending_machine',\n",
    " 'veterinary',\n",
    " 'waste_basket',\n",
    " 'waste_disposal',\n",
    " 'waste_transfer_station',\n",
    " 'wifi;telephone;device_charging_station',\n",
    "'_c0',\n",
    "'summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "merged = []\n",
    "\n",
    "for i in range(len(year)):\n",
    "    tc_df[i] = tc_df[i].join(weather, ['Issue_Date', 'Violation_Time'])\n",
    "    merged.append(tc_df[i].groupBy('prediction','Issue_Date','Violation_Time').count())\n",
    "    merged[i] = merged[i].join(weather, [\"Issue_Date\", \"Violation_Time\"])\n",
    "    merged[i] = merged[i].join(Amenity_per_location_2, (merged[i].prediction == Amenity_per_location_2.prediction)).drop(\"prediction\").drop(\"datetime\")\n",
    "    merged[i] = merged[i].withColumn(\"Issue_Date\", F.to_timestamp(merged[i].Issue_Date, 'MM/dd/yyyy'))\n",
    "    merged[i] = merged[i].withColumn('Day_of_week',F.dayofweek(merged[i].Issue_Date))\n",
    "    merged[i] = merged[i].withColumn('Day_of_year',F.dayofyear(merged[i].Issue_Date))\n",
    "    merged[i] = merged[i].withColumn('Day_of_month',F.dayofmonth(merged[i].Issue_Date))\n",
    "    merged[i] = merged[i].withColumn(\"Month\", F.date_format('Issue_Date', 'MM'))\n",
    "    merged[i] = merged[i].withColumn(\"Year\", F.date_format('Issue_Date', 'YYYY'))\n",
    "    merged[i] = merged[i].drop(\"Issue_Date\")\n",
    "    for c in amenities_to_drop:\n",
    "        merged[i] = merged[i].drop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# indexing categorical features\n",
    "\n",
    "# categorical_features = [\"icon\", \"precipType\", \"summary\"]\n",
    "categorical_features = [\"icon\", \"precipType\"]\n",
    "\n",
    "for cat in categorical_features:\n",
    "    for i in range(len(year)):\n",
    "        if i == 0:\n",
    "            stringIndexer = StringIndexer(inputCol=cat, outputCol=cat+\"_cat\", stringOrderType=\"frequencyDesc\")\n",
    "            stringIndexer.setHandleInvalid(\"keep\")\n",
    "            model = stringIndexer.fit(merged[i])\n",
    "            model.setHandleInvalid(\"keep\")\n",
    "        merged[i] = model.transform(merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dropping non-indexed categorical feature columns\n",
    "\n",
    "for i in range(len(year)):\n",
    "    merged[i] = merged[i].drop(\"icon\").drop(\"precipType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting the variables to floats for input into the regression model \n",
    "\n",
    "cat_features = [] \n",
    "\n",
    "floats = [x for x in merged[0].columns if x not in cat_features]\n",
    "\n",
    "for i in range(len(year)):\n",
    "    for feature in floats:\n",
    "        merged[i] = merged[i].withColumn(feature, merged[i][feature].cast(types.FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the final feature vector structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features used in the model are: \n",
      "root\n",
      " |-- Violation_Time: float (nullable = true)\n",
      " |-- count: float (nullable = false)\n",
      " |-- precipIntensity: float (nullable = true)\n",
      " |-- precipProbability: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- apparentTemperature: float (nullable = true)\n",
      " |-- dewPoint: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- pressure: float (nullable = true)\n",
      " |-- windSpeed: float (nullable = true)\n",
      " |-- windGust: float (nullable = true)\n",
      " |-- windBearing: float (nullable = true)\n",
      " |-- cloudCover: float (nullable = true)\n",
      " |-- uvIndex: float (nullable = true)\n",
      " |-- visibility: float (nullable = true)\n",
      " |-- precipAccumulation: float (nullable = true)\n",
      " |-- ozone: float (nullable = true)\n",
      " |-- Holiday: float (nullable = true)\n",
      " |-- atm: float (nullable = true)\n",
      " |-- bakery: float (nullable = true)\n",
      " |-- bank: float (nullable = true)\n",
      " |-- bar: float (nullable = true)\n",
      " |-- bbq: float (nullable = true)\n",
      " |-- bench: float (nullable = true)\n",
      " |-- bureau_de_change: float (nullable = true)\n",
      " |-- bus_station: float (nullable = true)\n",
      " |-- cafe: float (nullable = true)\n",
      " |-- childcare: float (nullable = true)\n",
      " |-- cinema: float (nullable = true)\n",
      " |-- clinic: float (nullable = true)\n",
      " |-- clothing store: float (nullable = true)\n",
      " |-- college: float (nullable = true)\n",
      " |-- doctors: float (nullable = true)\n",
      " |-- embassy: float (nullable = true)\n",
      " |-- events_venue: float (nullable = true)\n",
      " |-- fast_food: float (nullable = true)\n",
      " |-- gym: float (nullable = true)\n",
      " |-- hospital: float (nullable = true)\n",
      " |-- kindergarten: float (nullable = true)\n",
      " |-- marketplace: float (nullable = true)\n",
      " |-- money_transfer: float (nullable = true)\n",
      " |-- nightclub: float (nullable = true)\n",
      " |-- pharmacy: float (nullable = true)\n",
      " |-- place_of_worship: float (nullable = true)\n",
      " |-- post_office: float (nullable = true)\n",
      " |-- pub: float (nullable = true)\n",
      " |-- restaurant: float (nullable = true)\n",
      " |-- school: float (nullable = true)\n",
      " |-- supermarket: float (nullable = true)\n",
      " |-- Day_of_week: float (nullable = true)\n",
      " |-- Day_of_year: float (nullable = true)\n",
      " |-- Day_of_month: float (nullable = true)\n",
      " |-- Month: float (nullable = true)\n",
      " |-- Year: float (nullable = true)\n",
      " |-- icon_cat: float (nullable = false)\n",
      " |-- precipType_cat: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking feature vector types\n",
    "\n",
    "print('The features used in the model are: ')\n",
    "merged[0].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of features used in the model is: 56\n"
     ]
    }
   ],
   "source": [
    "# final number of features\n",
    "\n",
    "print('The final number of features used in the model is: '+ str(len(merged[0].columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameters \n",
    "\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RANDOM_SEED = 3\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vectors from the features for model input \n",
    "\n",
    "training_data, test_data = [], []\n",
    "\n",
    "for i in  range(len(year)):\n",
    "    feature_indices = [j for j, x in enumerate(merged[i].columns) if j!=1] # all columns except the label\n",
    "    assembler = VectorAssembler(inputCols=list(np.array(merged[i].columns)[np.array(feature_indices)]),outputCol=\"features\")\\\n",
    "                .setHandleInvalid(\"keep\")\n",
    "    spDF = assembler.transform(merged[i])\n",
    "    tr, te = spDF.randomSplit(splits, RANDOM_SEED)\n",
    "    training_data.append(tr)\n",
    "    test_data.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniting the vectors per year in a single dataframe\n",
    "\n",
    "train = training_data[0]\n",
    "test = test_data[0]\n",
    "\n",
    "for i in  range(1, len(year)):\n",
    "    train.union(training_data[i])\n",
    "    test.union(test_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean number of ticket per hour per location is: 4\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extracting the mean count\n",
    "\n",
    "mean_count = train.agg(F.avg(F.col(\"count\")))\n",
    "mean_count = mean_count.take(1)\n",
    "mean_count = mean_count[0][0]\n",
    "mean_count = int(mean_count)\n",
    "print('The mean number of ticket per hour per location is: ' + str(mean_count))\n",
    "test_withAvgCount = test.withColumn('avg_count', F.lit(mean_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 85.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# casting to float\n",
    "\n",
    "test_withAvgCount = test_withAvgCount.withColumn('avg_count', test_withAvgCount['avg_count'].cast(types.FloatType()))\n",
    "test_withAvgCount = test_withAvgCount.withColumn('count', test_withAvgCount['count'].cast(types.FloatType()))\n",
    "baseline = test_withAvgCount.select(['avg_count','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.175540554509165\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluating - RMSE\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"avg_count\", predictionCol=\"count\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(baseline)\n",
    "print('RMSE: '+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.055790505531438\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluating - MAE\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"avg_count\", predictionCol=\"count\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(baseline)\n",
    "print('MAE: '+str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# the following parameters should probably be pushed as far upwards as our machines can handle for better performance\n",
    "# training for 1 year with current parameters takes <10min\n",
    "\n",
    "RF_NUM_TREES = 40\n",
    "RF_MAX_DEPTH = 7\n",
    "RF_MAX_BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training RF model\n",
    "\n",
    "rf = RandomForestRegressor(numTrees=RF_NUM_TREES, maxDepth=RF_MAX_DEPTH, labelCol='count')\n",
    "rf.setSeed(RANDOM_SEED)\n",
    "model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting feature importance\n",
    "\n",
    "featureImp = model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    \"\"\"This function converts the extracted feature importance to a readable format by associating them to column names.\"\"\"\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting feature importance in readable format with column names\n",
    "\n",
    "scores = ExtractFeatureImp(featureImp, train, 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# saving feature importance to CSV\n",
    "\n",
    "scores.to_csv('rf_scores_whole_dataset_50_trees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                 name     score\n",
      "49   49          Day_of_year  0.155417\n",
      "0     0       Violation_Time  0.150248\n",
      "42   42     place_of_worship  0.090122\n",
      "51   51                Month  0.088712\n",
      "12   12              uvIndex  0.080956\n",
      "34   34            fast_food  0.047372\n",
      "48   48          Day_of_week  0.039022\n",
      "53   53             icon_cat  0.038392\n",
      "41   41             pharmacy  0.033262\n",
      "50   50         Day_of_month  0.023116\n",
      "46   46               school  0.023037\n",
      "31   31              doctors  0.022810\n",
      "43   43          post_office  0.018548\n",
      "22   22                bench  0.018518\n",
      "40   40            nightclub  0.014902\n",
      "20   20                  bar  0.013451\n",
      "27   27               cinema  0.012589\n",
      "45   45           restaurant  0.012133\n",
      "38   38          marketplace  0.011920\n",
      "36   36             hospital  0.011084\n",
      "25   25                 cafe  0.011027\n",
      "4     4  apparentTemperature  0.008448\n",
      "26   26            childcare  0.007706\n",
      "35   35                  gym  0.006552\n",
      "3     3          temperature  0.005737\n",
      "44   44                  pub  0.005450\n",
      "11   11           cloudCover  0.005151\n",
      "19   19                 bank  0.004391\n",
      "13   13           visibility  0.003950\n",
      "5     5             dewPoint  0.003846\n",
      "28   28               clinic  0.003602\n",
      "37   37         kindergarten  0.002912\n",
      "6     6             humidity  0.002761\n",
      "7     7             pressure  0.002748\n",
      "10   10          windBearing  0.002556\n",
      "1     1      precipIntensity  0.002161\n",
      "9     9             windGust  0.002083\n",
      "54   54       precipType_cat  0.002051\n",
      "8     8            windSpeed  0.001968\n",
      "2     2    precipProbability  0.001824\n",
      "30   30              college  0.001647\n",
      "17   17                  atm  0.001566\n",
      "24   24          bus_station  0.001231\n",
      "33   33         events_venue  0.000867\n",
      "47   47          supermarket  0.000573\n",
      "14   14   precipAccumulation  0.000534\n",
      "29   29       clothing store  0.000522\n",
      "39   39       money_transfer  0.000300\n",
      "21   21                  bbq  0.000089\n",
      "23   23     bureau_de_change  0.000078\n",
      "16   16              Holiday  0.000031\n",
      "18   18               bakery  0.000020\n",
      "32   32              embassy  0.000010\n",
      "15   15                ozone  0.000000\n",
      "52   52                 Year  0.000000\n"
     ]
    }
   ],
   "source": [
    "# printing the scores\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.60102\n",
      "Mean Absolute Error (MAE) on test data = 2.72508\n",
      "Wall time: 11min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluating\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# saving the trained model to file\n",
    "\n",
    "model_name = \"RF_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# defining model parameters\n",
    "\n",
    "MAX_ITERS = 10\n",
    "GBT_MAX_DEPTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training GBT model\n",
    "\n",
    "gbt = GBTRegressor(maxIter=MAX_ITERS, maxDepth=GBT_MAX_DEPTH, labelCol=\"count\", seed=RANDOM_SEED, leafCol=\"leafId\")\n",
    "model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting feature importance\n",
    "\n",
    "featureImp = model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting feature importance in readable format with column names\n",
    "\n",
    "scores = ExtractFeatureImp(featureImp, train, 'features')\n",
    "scores = scores.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# saving feature importance to CSV\n",
    "\n",
    "scores.to_csv('gbt_scores_whole_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                 name    score\n",
      "42   42     place_of_worship  0.20747\n",
      "12   12              uvIndex  0.13155\n",
      "31   31              doctors  0.07304\n",
      "19   19                 bank  0.06414\n",
      "34   34            fast_food  0.05487\n",
      "0     0       Violation_Time  0.05348\n",
      "41   41             pharmacy  0.05040\n",
      "20   20                  bar  0.04081\n",
      "43   43          post_office  0.03842\n",
      "36   36             hospital  0.03597\n",
      "49   49          Day_of_year  0.03217\n",
      "22   22                bench  0.02513\n",
      "48   48          Day_of_week  0.02511\n",
      "27   27               cinema  0.02013\n",
      "46   46               school  0.01915\n",
      "40   40            nightclub  0.01663\n",
      "26   26            childcare  0.01554\n",
      "45   45           restaurant  0.01132\n",
      "25   25                 cafe  0.01123\n",
      "28   28               clinic  0.00958\n",
      "51   51                Month  0.00936\n",
      "44   44                  pub  0.00922\n",
      "17   17                  atm  0.00902\n",
      "24   24          bus_station  0.00886\n",
      "53   53             icon_cat  0.00741\n",
      "33   33         events_venue  0.00326\n",
      "38   38          marketplace  0.00288\n",
      "50   50         Day_of_month  0.00261\n",
      "3     3          temperature  0.00217\n",
      "7     7             pressure  0.00162\n",
      "39   39       money_transfer  0.00136\n",
      "14   14   precipAccumulation  0.00126\n",
      "13   13           visibility  0.00083\n",
      "5     5             dewPoint  0.00081\n",
      "6     6             humidity  0.00062\n",
      "37   37         kindergarten  0.00054\n",
      "35   35                  gym  0.00048\n",
      "30   30              college  0.00040\n",
      "16   16              Holiday  0.00039\n",
      "21   21                  bbq  0.00035\n",
      "1     1      precipIntensity  0.00020\n",
      "11   11           cloudCover  0.00012\n",
      "23   23     bureau_de_change  0.00004\n",
      "10   10          windBearing  0.00003\n",
      "47   47          supermarket  0.00000\n",
      "52   52                 Year  0.00000\n",
      "2     2    precipProbability  0.00000\n",
      "8     8            windSpeed  0.00000\n",
      "4     4  apparentTemperature  0.00000\n",
      "15   15                ozone  0.00000\n",
      "32   32              embassy  0.00000\n",
      "9     9             windGust  0.00000\n",
      "29   29       clothing store  0.00000\n",
      "18   18               bakery  0.00000\n",
      "54   54       precipType_cat  0.00000\n"
     ]
    }
   ],
   "source": [
    "# printing the scores\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.367\n",
      "Mean Absolute Error (MAE) on test data = 2.63164\n",
      "Wall time: 13min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluating\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# saving the trained model to file\n",
    "\n",
    "model_name = \"GBT_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
