{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[2.0,1.0,1.0,1.0,...|\n",
      "|  0.0|[3.0,2.0,1.0,1.0,...|\n",
      "|  0.5|[1.0,2.0,3.0,1.0,...|\n",
      "|  2.0|[2.0,2.0,2.0,1.0,...|\n",
      "+-----+--------------------+\n",
      "\n",
      "Minimum population size of nodes after decision:\n",
      "1\n",
      "number of trees used:\n",
      "3\n",
      "Is bagging performed:\n",
      "True\n",
      "Decision strategy:\n",
      "onethird\n",
      "Model seed:\n",
      "42\n",
      "\n",
      "\n",
      "Feature importances (how much one feature influences the end prediction):\n",
      "(10,[0],[1.0])\n",
      "Output interpretation: Not sure if it's actually only deciding based on the first feature value or if this information is false\n",
      "\n",
      "\n",
      "Impossible input prediction:\n",
      "0.8333333333333334\n",
      "Leaf Id's obtained during prediction:\n",
      "[0.0,0.0,0.0]\n",
      "\n",
      "\n",
      "                                            features\n",
      "0  [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
      "model prediction:\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "from numpy import allclose\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pyspark\n",
    "\n",
    "# set folder to save/load model:\n",
    "temp_path = \"temp\"\n",
    "\n",
    "# start pyspark session:\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"RF model\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# example input data structure / data:\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense(2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)),\n",
    "    (0.0, Vectors.dense(3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)),\n",
    "    (0.5, Vectors.dense(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)),\n",
    "    (2.0, Vectors.dense(2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0))], [\"label\", \"features\"])\n",
    "\n",
    "# input structure of RF model: two column table, label and features. \n",
    "# Features consist of a vector containing all feature values for one object/location\n",
    "df.show()\n",
    "\n",
    "# Build the RF model:\n",
    "# numTrees = 128 https://link.springer.com/chapter/10.1007/978-3-642-31537-4_13 or \n",
    "# ~200 https://link.springer.com/content/pdf/10.1007/978-0-387-84858-7_15.pdf <-- RF book\n",
    "# maxDepth = None?  (we basically want to have as many leafs as possible to get the most accurate prediciton,\n",
    "# however, this is pretty much debateable as high depth trees are resource expensive)\n",
    "# featureSubsetStrategy = onethird (Used in regresssion, Set it to this instead of auto, \n",
    "# cuz I don't trust developers)\n",
    "rf = RandomForestRegressor(numTrees=3, maxDepth=5, featureSubsetStrategy='onethird')\n",
    "\n",
    "print(\"Minimum population size of nodes after decision:\")\n",
    "print(rf.getMinInstancesPerNode())\n",
    "\n",
    "rf.setSeed(42)\n",
    "\n",
    "# model training\n",
    "model = rf.fit(df)\n",
    "print(\"number of trees used:\")\n",
    "print(model.getNumTrees)\n",
    "\n",
    "\n",
    "print(\"Is bagging performed:\")\n",
    "print(model.getBootstrap())\n",
    "\n",
    "print(\"Decision strategy:\")\n",
    "print(model.getFeatureSubsetStrategy())\n",
    "\n",
    "print(\"Model seed:\")\n",
    "print(model.getSeed())\n",
    "\n",
    "model.setLeafCol(\"leafId\")\n",
    "print(\"\\n\")\n",
    "print(\"Feature importances (how much one feature influences the end prediction):\")\n",
    "print(model.featureImportances)\n",
    "print(\"Output interpretation: Not sure if it's actually only deciding based on the first feature value or \" + \n",
    "      \"if this information is false\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Weights assigned to each tree: (if not done should also assign 1.0 to all of them by default)\n",
    "allclose(model.treeWeights, [1.0, 1.0, 1.0])\n",
    "\n",
    "# test on impossible values:\n",
    "test0 = spark.createDataFrame([(Vectors.dense([-1.0, -1.0, -1.0]),)], [\"features\"])\n",
    "print(\"Impossible input prediction:\")\n",
    "print(model.predict(test0.head().features))\n",
    "print(\"Leaf Id's obtained during prediction:\")\n",
    "print(model.predictLeaf(test0.head().features))\n",
    "print(\"\\n\")\n",
    "result = model.transform(test0).head()\n",
    "result.prediction\n",
    "\n",
    "result.leafId\n",
    "\n",
    "model.numFeatures\n",
    "\n",
    "model.trees\n",
    "\n",
    "model.getNumTrees\n",
    "\n",
    "test1 = spark.createDataFrame([(Vectors.dense(1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0),)], [\"features\"])\n",
    "print(test1.toPandas())\n",
    "print(\"model prediction:\")\n",
    "print(model.transform(test1).head().prediction)\n",
    "\n",
    "# save a RF instance:\n",
    "#rfr_path = temp_path + \"/rfr\"\n",
    "#rf.save(rfr_path)\n",
    "# load a RF instance:\n",
    "#rf2 = RandomForestRegressor.load(rfr_path)\n",
    "#rf2.getNumTrees()\n",
    "\n",
    "# save the model:\n",
    "#model_path = temp_path + \"/rfr_model\"\n",
    "#model.save(model_path)\n",
    "# loaf the model:\n",
    "#model2 = RandomForestRegressionModel.load(model_path)\n",
    "#model.featureImportances == model2.featureImportances\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 1: 1.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Vectors.sparse(4, [1, 3], [3.0, 4.0])\n",
    "a\n",
    "b = Vectors.dense([1,2,3,4])\n",
    "b\n",
    "a + b\n",
    "Vectors.sparse(3, [0,1.0], [1, 1.0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
