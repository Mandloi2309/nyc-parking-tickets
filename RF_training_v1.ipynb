{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark session libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "## Required for parsing the file as csv\n",
    "import csv\n",
    "from io import StringIO\n",
    "from itertools import islice, repeat\n",
    "\n",
    "## For preprocessing\n",
    "from re import search, split, sub, compile as comp\n",
    "import numpy as np\n",
    "from statistics import median\n",
    "\n",
    "## For Plots\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib import cm, colors\n",
    "\n",
    "\n",
    "## for RF model\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import types\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from numpy import allclose\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. \n",
      "Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. \n",
      "To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. \n",
      "To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"SDDM\", master='local[*,4]')\n",
    "sc.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")\n",
    "#ss = SparkSession.builder.appName('SDDM_2').getOrCreate()\n",
    "#print(sc.pythonVer)\n",
    "#print (sc.master)\n",
    "#sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32g\")\\\n",
    "    .config(\"spark.executor.memory\", \"64g\")\\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCSV(csvRow) :\n",
    "    '''Parses a row into a list of elements '''\n",
    "    data = StringIO(csvRow)\n",
    "    dataReader = csv.reader(data, lineterminator = '')\n",
    "    return(next(dataReader))\n",
    "\n",
    "def readFileAsCSV(session, filepath):\n",
    "    '''Reads a files as text file and then parses each row and returns a list of list: \n",
    "        [[Row]\n",
    "         [Row]\n",
    "         [Row]]\n",
    "     '''\n",
    "    try:\n",
    "        data = session.textFile(name = str(filepath))\n",
    "        data = data.map(parseCSV)\n",
    "    except:\n",
    "        print('Failed to read the file!')\n",
    "        data = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticket data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tickTime(x, ind):\n",
    "    '''Extracts the month and year from the issue date'''\n",
    "    try:\n",
    "        m = int(x[ind][0:2])\n",
    "        y = int(x[ind][6:10])\n",
    "    except:\n",
    "        m = '0'\n",
    "        y = '0'    \n",
    "    x.append(str(m))\n",
    "    x.append(str(y))\n",
    "    return x\n",
    "\n",
    "def street_preprocess(x, ind):\n",
    "    '''Preprocess the street names'''\n",
    "    s = x[ind]\n",
    "    s = s.replace('AVENUE','AVE').replace('STREET','ST').replace('BLVD','BL')\n",
    "    s = s.replace('\\sEAST\\s',' E ').replace('\\sWEST\\s',' W ').replace('\\sNORTH\\s',' N ').replace('\\sSOUTH\\s',' S ')\n",
    "    s = s.replace('\\sROAD\\s',' RD ').replace('\\sEXPY\\s','EXWY').replace('\\sPARKWY\\s','PKWY').replace('\\sISLAND\\s','ISL')\n",
    "    s = s.replace('\\sFIRST\\s','1').replace('\\sSECOND\\s','2').replace('\\sTHRID\\s','3')\n",
    "    s = s.replace('\\sFOURTH\\s','4').replace('\\sFIVETH\\s','5').replace('\\sSIXTH\\s','6')\n",
    "    s = s.replace('\\sSEVENTH\\s','7').replace('\\sEIGHTH\\s','8').replace('\\sNINETH\\s','9').replace('\\sTENTH\\s','10')\n",
    "    s = s.split()\n",
    "    result = [x if not search(r'\\d', x) else sub('[^0-9]','', x) for x in s]\n",
    "    result = ' '.join(result)\n",
    "    x[ind] = result.lower()\n",
    "    return x\n",
    "\n",
    "def rState(x, ind):\n",
    "    if x[ind] == 'NY':\n",
    "        x.append('0')\n",
    "    else:\n",
    "        x.append('1')\n",
    "    return x\n",
    "\n",
    "def violationType(x, ind):\n",
    "    mydict = {\"Misc\":[35,41,90,91,94],\n",
    "                        \"No Parking\":[20,21,23,24,27],\n",
    "                        \"No Standing\":[3,4,5,6,8,10,11,12,13,14,15,16,17,18,19,22,25,26,30,31,40,44,54,57,58,63,64,77,78,81,89,92],\n",
    "                        \"Permit/Doc Issue\":[1,2,29,70,71,72,73,76,80,83,87,88,93,97],\n",
    "                        \"Plate Issues\":[74,75,82],\n",
    "                        \"Obstructing Path\":[7,9,36,45,46,47,48,49,50,51,52,53,55,56,59,60,61,62,66,67,68,79,84,96,98],\n",
    "                        \"Overtime\":[28,32,33,34,37,38,39,42,43,65,69,85,86]\n",
    "                        }\n",
    "    label = ''\n",
    "    try:\n",
    "        for key, value in mydict.items():\n",
    "             for y in value:\n",
    "                    if y == int(x[ind]):\n",
    "                        label = key\n",
    "    except: \n",
    "        label = ''\n",
    "    newLabs = {0:'',\n",
    "               1:\"Misc\",\n",
    "               2:\"No Parking\",\n",
    "               3:\"No Standing\",\n",
    "               4:\"Permit/Doc Issue\",\n",
    "               5:\"Plate Issues\",\n",
    "               6:\"Obstructing Path\",\n",
    "               7:\"Overtime\"}\n",
    "    x.append(str(list(newLabs.keys())[list(newLabs.values()).index(label)]))\n",
    "    x[ind] = label\n",
    "    return x\n",
    "    \n",
    "def sH(x, ind):\n",
    "    ''' Extracts the street number and house number from House number column\n",
    "        Some house numbers are: 123-34, 45-56 and some are 34, 45 etc\n",
    "    '''\n",
    "    house_num = x[ind]\n",
    "    try:\n",
    "        if house_num == '':\n",
    "            s = '0'\n",
    "            h = '0'\n",
    "        else:\n",
    "            cond = '-' in house_num\n",
    "            if cond:\n",
    "                s, h = house_num.split('-')\n",
    "            else:\n",
    "                s = int(house_num)\n",
    "                h = '0'\n",
    "    except:\n",
    "        s = '0'\n",
    "        h = '0'\n",
    "    x.append(str(s))\n",
    "    x.append(str(h))\n",
    "    return x\n",
    "    \n",
    "def preprocessedCSV(session, filepath):\n",
    "    '''\n",
    "    Reads the csv files, and then converts Issue date to date and month\n",
    "    '''\n",
    "    data = readFileAsCSV(session, filepath)\n",
    "    header = data.take(1)[0]\n",
    "    data = data.map(lambda x: [x[0], x[2], x[3], x[4], x[5], x[6], x[7], x[19], x[23], x[24]])\n",
    "    header = data.take(1)[0]\n",
    "    #print(header)\n",
    "    ## Extracting the month and year\n",
    "    data = data.map(lambda x: tickTime(x, header.index('Issue Date')))\n",
    "    header.append('Issue Month')\n",
    "    header.append('Issue Year')\n",
    "    \n",
    "    ## Removing the header line\n",
    "    data = data.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "    ## Preprocessing the street names\n",
    "    data = data.map(lambda x: street_preprocess(x, header.index('Street Name')))    \n",
    "    ## Extracts the street and house number\n",
    "    data = data.map(lambda x: sH(x, header.index('House Number')))\n",
    "    header.append('Street')\n",
    "    header.append('House')\n",
    "    data = data.map(lambda x: rState(x, header.index('Registration State')))\n",
    "    header.append('RState')\n",
    "    data = data.map(lambda x: violationType(x, header.index('Violation Code')))\n",
    "    header.append('VType')\n",
    "    return data, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streetHouse(x):\n",
    "    '''\n",
    "        Extracts the street and house number ranges for a street name and coordinate\n",
    "    '''\n",
    "    house_num = x[0]\n",
    "    try:\n",
    "        if house_num == '':\n",
    "            l_s_min = '0'\n",
    "            l_s_max = '0'\n",
    "            r_s_min = '0'\n",
    "            r_s_max = '0'\n",
    "\n",
    "            h_l_min = '0'\n",
    "            h_l_max = '0'\n",
    "            h_r_min = '0'\n",
    "            h_r_max = '0'\n",
    "        else:\n",
    "            cond = '-' in house_num\n",
    "            if cond:\n",
    "                l_s_min, h_l_min = x[0].split('-')\n",
    "                l_s_max, h_l_max = x[1].split('-')\n",
    "                r_s_min, h_r_min = x[4].split('-')\n",
    "                r_s_max, h_r_max = x[5].split('-')\n",
    "            else:\n",
    "                l_s_min = int(x[0])\n",
    "                l_s_max = int(x[1])\n",
    "                r_s_min = int(x[4])\n",
    "                r_s_max = int(x[5])\n",
    "                h_l_min = '0'\n",
    "                h_l_max = '0'\n",
    "                h_r_min = '0'\n",
    "                h_r_max = '0'\n",
    "\n",
    "    except:\n",
    "        l_s_min = '0'\n",
    "        l_s_max = '0'\n",
    "        r_s_min = '0'\n",
    "        r_s_max = '0'\n",
    "        h_l_min = '0'\n",
    "        h_l_max = '0'\n",
    "        h_r_min = '0'\n",
    "        h_r_max = '0'\n",
    "        \n",
    "    x.extend([l_s_min, l_s_max, r_s_min, r_s_max, h_l_min, h_l_max, h_r_min, h_r_max])\n",
    "    return x\n",
    "\n",
    "def geoms(x, ind):\n",
    "    '''Extracting one single latitute and longitude values from the geometry '''\n",
    "    coords = x[ind]\n",
    "    try: \n",
    "        coords = coords.replace('MULTILINESTRING ', '').replace('(','').replace(')', '').split(', ')\n",
    "        coords = [i.split(' ') for i in coords]\n",
    "        coords = [[float(j), float(k)] for j,k in coords]\n",
    "        lon = str(median([j for j,k in coords ]))\n",
    "        lat = str(median([k for j,k in coords ]))\n",
    "    except:\n",
    "        lon = 'NA'\n",
    "        lat = 'NA'\n",
    "    x.append(lon)\n",
    "    x.append(lat)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def createCoordsFiles(session, filepath):\n",
    "    '''Reading the centerline data set and returns a preprocessed RDD'''\n",
    "    coords = readFileAsCSV(session, filepath) \n",
    "    ## Removing the first line\n",
    "    coords = coords.map(lambda x: [x[0],x[1],x[28],x[3],x[4],x[5]])\n",
    "    ## Getting the header\n",
    "    coords_header = coords.take(1)[0]\n",
    "    coords = coords.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "    ## Preprocessing the street names\n",
    "    coords = coords.map(lambda x: street_preprocess(x, coords_header.index('FULL_STREE'))) \n",
    "    ## Finding the street numbers and house number limits\n",
    "    coords = coords.map(streetHouse)\n",
    "    coords = coords.map(lambda x: geoms(x, coords_header.index('the_geom')))\n",
    "    ## Extracting the required columns\n",
    "    #coords = coords.map(lambda x: [x[2],x[6],x[7],x[8],x[9], x[10], x[11]])\n",
    "    coords_header.extend(['L_S_min', 'L_S_max', 'R_S_min', 'R_S_max', 'L_H_min', 'L_H_max', 'R_H_min', 'R_H_max', 'lon', 'lat'] )\n",
    "    return coords, coords_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the ticket to their respectve coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchstreet(t, c):\n",
    "    '''\n",
    "    Based on the street and house number in tickets dataframe finds the coordintae value in centerline dataframe\n",
    "    and merges the coordinate value to it.\n",
    "    '''\n",
    "    ## Making the columns integers for comparision\n",
    "    t =  t.withColumn(\"Issue_Month\", t[\"Issue_Month\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Issue_Year\", t[\"Issue_Year\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Street\", t[\"Street\"].cast('integer'))\n",
    "    t =  t.withColumn(\"House\", t[\"House\"].cast('integer'))\n",
    "    t =  t.withColumn(\"RState\", t[\"RState\"].cast('integer'))\n",
    "    t =  t.withColumn(\"VType\", t[\"VType\"].cast('integer'))\n",
    "    t =  t.withColumn(\"Ids\", monotonically_increasing_id())\n",
    "    t =  t.withColumn(\"Ids\", t[\"Ids\"].cast('string'))\n",
    "    \n",
    "    c = c.select('FULL_STREE', c.L_S_min.cast('integer'),c.L_S_max.cast('integer'),\\\n",
    "                 c.R_S_min.cast('integer'),c.R_S_max.cast('integer'),\\\n",
    "                 c.L_H_min.cast('integer'),c.L_H_max.cast('integer'),\\\n",
    "                 c.R_H_min.cast('integer'),c.R_H_max.cast('integer'),\\\n",
    "                 c.lon.cast('float'),c.lat.cast('float'))\n",
    "    ## performs inner join on the tickets. \n",
    "    merged = t.join(c, [t.Street_Name == c.FULL_STREE,\\\n",
    "                        (t.Street>=c.L_S_min)  | (t.Street>=c.R_S_min),\\\n",
    "                        (t.Street<=c.L_S_max)  | (t.Street<=c.R_S_max),\\\n",
    "                        (t.House >=c.L_H_min)  | (t.House >=c.R_H_min),\\\n",
    "                        (t.House <= c.L_H_max) | (t.House <= c.R_H_max)],'inner').select('Ids', 'Summons_Number', 'Registration_State', 'Plate_Type', 'Issue_Date', 'Violation_Code', 'Vehicle_Body_Type', 'Vehicle_Make', 'House_Number', 'Street_Name', 'Issue_Month', 'Issue_Year', 'Violation_Time', 'Street', 'House', 'RState', 'VType', 'lon', 'lat')\n",
    "    \n",
    "    return merged.dropDuplicates(subset=['Ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(sc, filepath, filename, year):\n",
    "    '''Reads each file in a loop and returns a list of RDDs'''\n",
    "    tickets = []\n",
    "    for yr in year:\n",
    "        filelocation = str(filepath)+str(filename)+str(yr)+\".csv\"\n",
    "        print(filelocation)\n",
    "        parking_data, header = preprocessedCSV(sc, filelocation)\n",
    "        tickets.append(parking_data)\n",
    "    return tickets, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(data, Val, Key):\n",
    "    pairs = data.map(lambda x: (x[Key], x[Val]))\n",
    "    return pairs.groupByKey().collect()\n",
    "\n",
    "def group_data_toList(y, groupby):\n",
    "    '''\n",
    "    Input:\n",
    "        - y: Grouped pyspark data returned from group_data function\n",
    "        - groupby: Column name to remove an extra element\n",
    "    '''\n",
    "    lab = list(map(lambda x:x[0], y))\n",
    "    val = list(map(lambda x:len(x[1]), y))\n",
    "    try:\n",
    "        kick = lab.index(groupby)\n",
    "        lab.pop(kick)\n",
    "        val.pop(kick)\n",
    "    except:\n",
    "        0\n",
    "    if groupby=='Issue Month' or groupby == 'Violation Code':\n",
    "        lab = list(map(lambda x: int(x), lab))\n",
    "    return [lab,val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ax, header, data, year, groupby, pt, plot_data):\n",
    "    '''\n",
    "    Input:\n",
    "        - data: PySpark parsed CSV\n",
    "        - groupby: Column name for grouping the data \n",
    "        - pt: Plot type\n",
    "        - plot data: Variables required for bar plot\n",
    "        \n",
    "    Output: \n",
    "        - Plot\n",
    "    '''\n",
    "    if pt=='bar':\n",
    "        col, val, axis_labels, legend_labels = plot_data\n",
    "        \n",
    "        data = data[data.Issue_Year == year]\n",
    "        \n",
    "        cat_index = data.columns.get_loc(col)\n",
    "        cat1 = data[data[cat_index] == val]\n",
    "        cat2 = data[data[cat_index] != val]\n",
    "        cat2 = cat2.groupby('Issue_Month').sum() \n",
    "        print(cat2)\n",
    "        ## plotting the graph\n",
    "        ax.bar(cat1.Issue_Month, cat1[:, 4], width = 0.5, label=legend_labels[0]) \n",
    "        ax.bar(cat2.Issue_Month, cat2[:, 4], width = 0.5, label=legend_labels[1])\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.set_xlabel(groupby, fontsize=18)\n",
    "        ax.set_ylabel('Number of tickets', fontsize=18)\n",
    "        ax.set_title('Parking tickets for the year '+str(year-1), fontsize=22)\n",
    "    \n",
    "    if pt == 'pie':\n",
    "        #try:\n",
    "            #groupby_index = header.index(groupby)\n",
    "            #count_column = header.index('Summons Number')\n",
    "            \n",
    "            label, data = group_data_toList(group_data(data , Val=count_column, Key=groupby_index), groupby)\n",
    "            title = 'Parking ticket '+str(groupby)+' for the year '+str(year-1)\n",
    "            \n",
    "            if groupby == 'Violation Code': \n",
    "                mydict = { 0:'',\n",
    "                           1:\"Misc\",\n",
    "                           2:\"No Parking\",\n",
    "                           3:\"No Standing\",\n",
    "                           4:\"Permit/Doc Issue\",\n",
    "                           5:\"Plate Issues\",\n",
    "                           6:\"Obstructing Path\",\n",
    "                           7:\"Overtime\"}\n",
    "                ## Defining the violation code merges as a dictionary\n",
    "                labs = {\"Misc\":[35,41,90,91,94],\n",
    "                        \"No Parking\":[20,21,23,24,27],\n",
    "                        \"No Standing\":[3,4,5,6,8,10,11,12,13,14,15,16,17,18,19,22,25,26,30,31,40,44,54,57,58,63,64,77,78,81,89,92],\n",
    "                        \"Permit/Doc Issue\":[1,2,29,70,71,72,73,76,80,83,87,88,93,97],\n",
    "                        \"Plate Issues\":[74,75,82],\n",
    "                        \"Obstructing Path\":[7,9,36,45,46,47,48,49,50,51,52,53,55,56,59,60,61,62,66,67,68,79,84,96,98],\n",
    "                        \"Overtime\":[28,32,33,34,37,38,39,42,43,65,69,85,86]\n",
    "                        }\n",
    "                ## Count based on the grouping\n",
    "            \n",
    "                temp = defaultdict(list)\n",
    "                for i in range(len(label)):\n",
    "                    for key, val in labs.items():\n",
    "                        if label[i] in val:\n",
    "                            if temp[key] == []:\n",
    "                                temp[key] = 0\n",
    "                            else:\n",
    "                                temp[key] = temp[key]+data[i]\n",
    "\n",
    "                ## Ordering data based on the dictionary \n",
    "                label, data  = list(), list()\n",
    "                for key in labs.keys():\n",
    "                    label.append(key)\n",
    "                    data.append(temp[key])    \n",
    "\n",
    "            ## Defining color for each category\n",
    "            temp = defaultdict(list)\n",
    "            for l,c in zip(labs,cm.tab20(range(len(labs)))):\n",
    "                temp[l]=c\n",
    "\n",
    "            centre_circle = plt.Circle((0,0),0.85,fc='white') ## radius to make it like a donut\n",
    "            explode = np.full(len(label), 0.04) ## Gaps between the categories\n",
    "\n",
    "            pat = ax.pie(list(map(lambda x: x*100/sum(data), data)), labels=label, textprops={'fontsize': 20}, autopct='%1.1f%%', startangle=90, pctdistance=0.6, explode = explode)\n",
    "            if groupby == 'Violation Code':\n",
    "                for pie_wedge in pat[0]:\n",
    "                    pie_wedge.set_edgecolor('white')\n",
    "                    pie_wedge.set_facecolor(temp[pie_wedge.get_label()]) # Assigning color code for each catergory\n",
    "\n",
    "            ax.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "            ax.set_title(title, fontsize =22, pad=20)\n",
    "            plt.gcf().gca().add_artist(centre_circle)\n",
    "        #except:\n",
    "        #    print('Failed to plot!')        \n",
    "    return ax \n",
    "\n",
    "\n",
    "def EDA(header, tickets, year, groupby, pt, plotdim, plot_data):\n",
    "    fig = plt.figure(figsize=(30, 25))\n",
    "    axs=plt.GridSpec(plotdim[0], plotdim[1], hspace=0.15, wspace=0.1)\n",
    "    tickets = tickets.select('Issue_Year', 'Issue_Month', str(plot_data[0])).groupBy('Issue_Year', 'Issue_Month', plot_data[0]).count().toPandas()\n",
    "    for i in range(len(year)):\n",
    "        tickets.filter(lambda x: x)\n",
    "        plot(fig.add_subplot(axs[i]), header, tickets, year[i], groupby, pt, plot_data)\n",
    "    plt.savefig('EDA_'+str(groupby)+'_'+str(pt)+'.png',  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final showdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the ticket with their coordinate values\n",
    "This is done using join method\n",
    "First we convert the `all_tickets` into a dataframe, then we read the cetnerline data set as a dataframe `coord_df` and then using `matchStreet` function we match the coordinates based on the left and right range of street numbers and house numbers based on the street names. \n",
    "Then we select the data which is from the years of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyse(year, filepath, filename):\n",
    "    ## Returns a list of RDDs, these RDD have data for each year\n",
    "    tickets, header = getData(sc, filepath, filename, year)\n",
    "    ## Merging the RDDs into a  one single RDD\n",
    "    ticket_part0 = tickets[0]\n",
    "    #ticket_part1 = tickets[1]\n",
    "    #ticket_part2 = tickets[2]\n",
    "    #ticket_part3 = tickets[3]\n",
    "    #ticket_part4 = tickets[4]\n",
    "    #ticket_part5 = tickets[5]\n",
    "    \n",
    "    ## Converting the RDD list to a pyspark dataframe\n",
    "    ticket_part0 = ss.createDataFrame(ticket_part0, schema=[x.replace(' ', '_') for x in header])\n",
    "    #ticket_part1 = ss.createDataFrame(ticket_part1, schema=[x.replace(' ', '_') for x in header])\n",
    "    #ticket_part2 = ss.createDataFrame(ticket_part2, schema=[x.replace(' ', '_') for x in header])\n",
    "    #ticket_part3 = ss.createDataFrame(ticket_part3, schema=[x.replace(' ', '_') for x in header])\n",
    "    #ticket_part4 = ss.createDataFrame(ticket_part4, schema=[x.replace(' ', '_') for x in header])\n",
    "    #ticket_part5 = ss.createDataFrame(ticket_part5, schema=[x.replace(' ', '_') for x in header])\n",
    "    ## Reading the centerline data set\n",
    "    coords, c_header = createCoordsFiles(sc, 'Centerline.csv')\n",
    "    coord_df = ss.createDataFrame(coords, schema=c_header)\n",
    "    \n",
    "    ## Matching the tickets to their coordinate location\n",
    "    ticket_part0 = matchstreet(ticket_part0, coord_df)\n",
    "    #ticket_part1 = matchstreet(ticket_part1, coord_df)\n",
    "    #ticket_part2 = matchstreet(ticket_part2, coord_df)\n",
    "    #ticket_part3 = matchstreet(ticket_part3, coord_df)\n",
    "    #ticket_part4 = matchstreet(ticket_part4, coord_df)\n",
    "    #ticket_part5 = matchstreet(ticket_part5, coord_df)\n",
    "    \n",
    "    ## Keeping the dataset which is from 2015 to 2020\n",
    "    ticket_part0 = ticket_part0.filter(F.col('Issue_Year').isin(year))\n",
    "    #ticket_part1 = ticket_part1.filter(F.col('Issue_Year').isin(year))\n",
    "    #ticket_part2 = ticket_part2.filter(F.col('Issue_Year').isin(year))\n",
    "    #ticket_part3 = ticket_part3.filter(F.col('Issue_Year').isin(year))\n",
    "    #ticket_part4 = ticket_part4.filter(F.col('Issue_Year').isin(year))\n",
    "    #ticket_part5 = ticket_part5.filter(F.col('Issue_Year').isin(year))\n",
    "    \n",
    "    #return ticket_part0, ticket_part1,ticket_part2,ticket_part3,ticket_part4,ticket_part5\n",
    "    return ticket_part0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filepath = 'nyc-parking-tickets/'\n",
    "filename = 'Parking_Violations_Issued_-_Fiscal_Year_'\n",
    "year = list(range(2015, 2021))\n",
    "\n",
    "# this is set for reading only the first year right now\n",
    "year=[2015]\n",
    "\n",
    "#t_part1, t_part2,t_part3,t_part4,t_part5,t_part6 = Analyse(year, filepath, filename)\n",
    "t_part1= Analyse(year, filepath, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"lon\"], outputCol=\"features\")\n",
    "t_part1_with_location = vecAssembler.transform(t_part1)\n",
    "#t_part2_with_location = vecAssembler.transform(t_part2)\n",
    "#t_part3_with_location = vecAssembler.transform(t_part3)\n",
    "#t_part4_with_location = vecAssembler.transform(t_part4)\n",
    "#t_part5_with_location = vecAssembler.transform(t_part5)\n",
    "#t_part6_with_location = vecAssembler.transform(t_part6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfrom pyspark.ml.clustering import BisectingKMeans\\nbkm = BisectingKMeans().setK(796).setSeed(1)\\nmodel = bkm.fit(t_part1_with_location.select('features'))\\ntransformed_1 = model.transform(t_part1_with_location)\\nmodel = bkm.fit(t_part2_with_location.select('features'))\\ntransformed_2 = model.transform(t_part2_with_location)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=796, seed=1)  #  clusters here\n",
    "\n",
    "model = kmeans.fit(t_part1_with_location.select('features'))\n",
    "transformed_1 = model.transform(t_part1_with_location)\n",
    "#transformed_2 = model.transform(t_part2_with_location)\n",
    "#transformed_3 = model.transform(t_part3_with_location)\n",
    "#transformed_4 = model.transform(t_part4_with_location)\n",
    "#transformed_5 = model.transform(t_part5_with_location)\n",
    "#transformed_6 = model.transform(t_part6_with_location)\n",
    "'''\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkm = BisectingKMeans().setK(796).setSeed(1)\n",
    "model = bkm.fit(t_part1_with_location.select('features'))\n",
    "transformed_1 = model.transform(t_part1_with_location)\n",
    "model = bkm.fit(t_part2_with_location.select('features'))\n",
    "transformed_2 = model.transform(t_part2_with_location)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ctr=[]\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    ctr.append(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ctr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "mc = MarkerCluster()\n",
    "m = folium.Map(\n",
    "    location=[40.767937,-73.982155],\n",
    "    zoom_start=12,)\n",
    "\n",
    "\n",
    "for i in range(len(ctr)):\n",
    "    mc.add_child(folium.Marker(location=[ ctr[i][0],ctr[i][1] ]))\n",
    "\n",
    "\n",
    "m.add_child(mc)\n",
    "m.save('F:/marker_cluster_example_file.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "amenity_names = []\n",
    "amenity_count = []\n",
    "import requests, json, time, random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "Amenity_per_location =pd.DataFrame()\n",
    "Amenity_per_location_2=pd.DataFrame()\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "for i in range(len(ctr)):\n",
    "    overpass_query = '[out:json];' +'(' + \\\n",
    "    '// query part for: “aminity=*”' +'\\n'+\\\n",
    "    'node[\"amenity\"](around:1000,' + str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    'way[\"amenity\"](around:1000,' +  str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    'relation[\"amenity\"](around:1000,' +  str(ctr[i][0])+','+str(ctr[i][1])+');'+'\\n'+\\\n",
    "    ');' + '\\n'+\\\n",
    "    '// print results'+'\\n'+\\\n",
    "    'out;'+'\\n'+\\\n",
    "    '>;'+'out count;'\n",
    "     \n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    try:\n",
    "        data  = response.json()\n",
    "    \n",
    "    except (requests.exceptions.ConnectionError, json.decoder.JSONDecodeError):\n",
    "        time.sleep(2**1 + random.random()*0.01) #exponential backoff\n",
    "\n",
    "\n",
    "    typeamenity =[]\n",
    "    for l in range(len(data['elements'])):\n",
    "        try:\n",
    "            ind = list(data['elements'][l].keys()).index('tags')\n",
    "        \n",
    "            typeamenity.append(data['elements'][l]['tags']['amenity'])\n",
    "        except:\n",
    "            ind = False\n",
    "\n",
    "    amenity_names.append(list(Counter(typeamenity).keys()))\n",
    "    amenity_count.append(list(Counter(typeamenity).values()))\n",
    "    dictionary = dict(zip(amenity_names[i], amenity_count[i]))\n",
    "    Amenity_per_location =  pd.DataFrame.from_dict(dictionary, orient='index')\n",
    "    Amenity_per_location_2 = pd.concat([Amenity_per_location, Amenity_per_location_2], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is how each ticket file look like\n",
    "\n",
    "Each ticket will have multiple columns below you can see some of the columns along with the coordinate of each ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenity_per_location_2 = Amenity_per_location_2.T\n",
    "Amenity_per_location_2['prediction'] =np.arange(0,len(Amenity_per_location_2))\n",
    "Amenity_per_location_2.to_csv('F:/Amenity_per_location_2_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Amenity_per_location_2 = ss.read.csv(\"Amenity_per_location.csv\", header=True, sep=\",\");\n",
    "Location_with_features_1 = Amenity_per_location_2.join(transformed_1, on=['prediction'], how='right_outer')\n",
    "#Location_with_features_2 = Amenity_per_location_2.join(transformed_2, on=['prediction'], how='right_outer')\n",
    "#Location_with_features_3 = Amenity_per_location_2.join(transformed_2, on=['prediction'], how='right_outer')\n",
    "#Location_with_features_4 = Amenity_per_location_2.join(transformed_2, on=['prediction'], how='right_outer')\n",
    "#Location_with_features_5 = Amenity_per_location_2.join(transformed_2, on=['prediction'], how='right_outer')\n",
    "#Location_with_features_6 = Amenity_per_location_2.join(transformed_2, on=['prediction'], how='right_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location_with_features_1 = Location_with_features_1.withColumn(\"Violation_Time\", F.concat(F.col(\"Violation_Time\"), F.lit(\"M\")))\n",
    "Location_with_features_1 = Location_with_features_1.withColumn(\"Violation_Time\", F.to_timestamp(Location_with_features_1.Violation_Time, 'KKmmaa'))\n",
    "Location_with_features_1 = Location_with_features_1.withColumn(\"Violation_Time\", F.date_format('Violation_Time', 'HH'))\n",
    "Location_with_features_1 = Location_with_features_1.filter(Location_with_features_1.Violation_Time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelocation_weather = 'merged_weather_holidays_fixedMissingValues.csv'\n",
    "weather = ss.read.csv(filelocation_weather, header=True)\n",
    "weather = weather.withColumn(\"time\", F.to_timestamp(weather.time, 'HH:mm:ss'))\n",
    "weather = weather.withColumn(\"date\", F.to_timestamp(weather.date, 'yyyy-MM-dd'))\n",
    "weather = weather.withColumn(\"time\", F.date_format('time', 'HH'))\n",
    "weather = weather.withColumn(\"date\", F.date_format('date', 'MM/dd/yyyy'))\n",
    "weather = weather.withColumnRenamed(\"time\", \"Violation_Time\")\n",
    "weather = weather.withColumnRenamed(\"date\", \"Issue_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_locationFeatures_joined = Location_with_features_1.join(weather, (Location_with_features_1.Issue_Date == weather.date) & (Location_with_features_1.Violation_Time == weather.time))\n",
    "weather_locationFeatures_joined = Location_with_features_1.join(weather, ['Issue_Date', 'Violation_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = weather_locationFeatures_joined.groupBy('prediction','Issue_Date','Violation_Time').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_merge = agg.join(weather, [\"Issue_Date\", \"Violation_Time\"])\n",
    "re_merge_2 = re_merge.join(Amenity_per_location_2, (re_merge.prediction == Amenity_per_location_2.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_merge_2 = re_merge_2.drop(\"prediction\")\n",
    "re_merge_2 = re_merge_2.drop(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_merge_2 = re_merge_2.withColumn(\"Issue_Date\", F.to_timestamp(re_merge_2.Issue_Date, 'MM/dd/yyyy'))\n",
    "re_merge_2 = re_merge_2.withColumn('Day_of_week',F.dayofweek(re_merge_2.Issue_Date))\n",
    "re_merge_2 = re_merge_2.withColumn('Day_of_year',F.dayofyear(re_merge_2.Issue_Date))\n",
    "re_merge_2 = re_merge_2.withColumn('Day_of_month',F.dayofmonth(re_merge_2.Issue_Date))\n",
    "re_merge_2 = re_merge_2.withColumn(\"Month\", F.date_format('Issue_Date', 'MM'))\n",
    "re_merge_2 = re_merge_2.withColumn(\"Year\", F.date_format('Issue_Date', 'YYYY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_merge_2 = re_merge_2.drop(\"Issue_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are chosen pretty recklessly. the ones i assumed would have the least correlation to parking tickets\n",
    "\n",
    "amenities_to_drop = ['amenity|ice_cream', \n",
    "'animal_boarding', \n",
    "'animal_shelter', \n",
    "'art_centre', \n",
    "'arts_centre', \n",
    "'bicycle_parking',\n",
    "'bicycle_rental', \n",
    "'bicycle_repair_station', \n",
    "'biergarten', \n",
    "'boat_rental', \n",
    "'boat_storage',\n",
    "'car_rental', \n",
    "'car_service', \n",
    "'car_sharing',\n",
    " 'car_wash',\n",
    " 'charging_station',\n",
    " 'clock',\n",
    " 'community_centre',\n",
    " 'compressed_air',\n",
    " 'concert_hall',\n",
    " 'cooking_school',\n",
    " 'courthouse',\n",
    " 'coworking_space',\n",
    " 'dancing_school',\n",
    " 'dentist',\n",
    " 'disused',\n",
    " 'dojo',\n",
    " 'drinking_water',\n",
    " 'driving_school',\n",
    " 'ferry_terminal',\n",
    " 'fire_station',\n",
    " 'food_court',\n",
    " 'fortune_teller',\n",
    " 'fountain',\n",
    " 'fuel',\n",
    " 'graphic_design',\n",
    " 'grave_yard',\n",
    " 'ice_cream',\n",
    " 'internet_cafe',\n",
    " 'karaoke_box',\n",
    " 'language_school',\n",
    " 'library',\n",
    " 'loading_dock',\n",
    " 'meditation_centre',\n",
    " 'monastery',\n",
    " 'motorcycle_parking',\n",
    " 'museum',\n",
    " 'music_school',\n",
    " 'music_venue',\n",
    " 'nail salon',\n",
    " 'nail_salon',\n",
    " 'nursing_home',\n",
    " 'outdoor_seating',\n",
    " 'parking',\n",
    " 'parking_entrance',\n",
    " 'parking_space',\n",
    " 'payment_centre',\n",
    " 'payment_terminal', 'picnic_table',\n",
    " 'police',\n",
    " 'post_box',\n",
    " 'post_depot',\n",
    " 'prep_school',\n",
    " 'prison',\n",
    " 'public_bath',\n",
    " 'public_bookcase',\n",
    " 'public_building',\n",
    " 'radio station',\n",
    " 'ranger_station',\n",
    " 'recycling',\n",
    " 'rescue_station',\n",
    " 'research_institute',\n",
    " 'salon',\n",
    " 'self_storage',\n",
    " 'shelter',\n",
    " 'shoe_repair',\n",
    " 'smoking_area',\n",
    " 'social_centre',\n",
    " 'social_facility',\n",
    " 'spa',\n",
    " 'stock_exchange',\n",
    " 'stripclub',\n",
    " 'studio', 'swimming_pool',\n",
    " 'swingerclub',\n",
    " 'taxi',\n",
    " 'telephone',\n",
    " 'theatre',\n",
    " 'toilets',\n",
    " 'tourism',\n",
    " 'townhall',\n",
    " 'training',\n",
    " 'university',\n",
    " 'urgent_care',\n",
    " 'vehicle_inspection',\n",
    " 'vending_machine',\n",
    " 'veterinary',\n",
    " 'waste_basket',\n",
    " 'waste_disposal',\n",
    " 'waste_transfer_station',\n",
    " 'wifi;telephone;device_charging_station',\n",
    "'_c0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in amenities_to_drop:\n",
    "    re_merge_2 = re_merge_2.drop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# indexing categorical features\n",
    "\n",
    "categorical_features = [\"summary\", \"icon\", \"precipType\"]\n",
    "\n",
    "for cat in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=cat, outputCol=cat+\"_cat\", stringOrderType=\"frequencyDesc\")\n",
    "    stringIndexer.setHandleInvalid(\"keep\")\n",
    "    model = stringIndexer.fit(re_merge_2)\n",
    "    model.setHandleInvalid(\"keep\")\n",
    "    re_merge_2 = model.transform(re_merge_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Violation_Time',\n",
       " 'count',\n",
       " 'precipIntensity',\n",
       " 'precipProbability',\n",
       " 'temperature',\n",
       " 'apparentTemperature',\n",
       " 'dewPoint',\n",
       " 'humidity',\n",
       " 'pressure',\n",
       " 'windSpeed',\n",
       " 'windGust',\n",
       " 'windBearing',\n",
       " 'cloudCover',\n",
       " 'uvIndex',\n",
       " 'visibility',\n",
       " 'precipAccumulation',\n",
       " 'ozone',\n",
       " 'Holiday',\n",
       " 'atm',\n",
       " 'bakery',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'bbq',\n",
       " 'bench',\n",
       " 'bureau_de_change',\n",
       " 'bus_station',\n",
       " 'cafe',\n",
       " 'childcare',\n",
       " 'cinema',\n",
       " 'clinic',\n",
       " 'clothing store',\n",
       " 'college',\n",
       " 'doctors',\n",
       " 'embassy',\n",
       " 'events_venue',\n",
       " 'fast_food',\n",
       " 'gym',\n",
       " 'hospital',\n",
       " 'kindergarten',\n",
       " 'marketplace',\n",
       " 'money_transfer',\n",
       " 'nightclub',\n",
       " 'pharmacy',\n",
       " 'place_of_worship',\n",
       " 'post_office',\n",
       " 'pub',\n",
       " 'restaurant',\n",
       " 'school',\n",
       " 'supermarket',\n",
       " 'Day_of_week',\n",
       " 'Day_of_year',\n",
       " 'Day_of_month',\n",
       " 'Month',\n",
       " 'Year',\n",
       " 'summary_cat',\n",
       " 'icon_cat',\n",
       " 'precipType_cat']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping non-indexed categorical feature columns\n",
    "\n",
    "re_merge_2 = re_merge_2.drop(\"summary\")\n",
    "re_merge_2 = re_merge_2.drop(\"icon\")\n",
    "re_merge_2 = re_merge_2.drop(\"precipType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_merge_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_features = [\"summary_cat\", \"icon_cat\", \"precipType_cat\", \"summary\", \"icon\", \"precipType\"]\n",
    "\n",
    "# the above line is commented since\n",
    "# the stringindexed categorical vars can also be float and not double\n",
    "\n",
    "cat_features = [] \n",
    "floats = [x for x in re_merge_2.columns if x not in cat_features]\n",
    "for feature in floats:\n",
    "    re_merge_2 = re_merge_2.withColumn(feature, re_merge_2[feature].cast(types.FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [i for i, x in enumerate(re_merge_2.columns) if i!=1] # all columns except the label\n",
    "\n",
    "# transformed_df contains the data as a list of LabeledPoints which contain the label and a vector of features\n",
    "# that is the format needed for the RF\n",
    "transformed_df = re_merge_2.rdd.map(lambda row: LabeledPoint(row[1], Vectors.dense(np.array(row)[np.array(feature_indices)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RANDOM_SEED = 3\n",
    "\n",
    "# in general, these should probably be pushed as far upwards as our machines can handle for better performance\n",
    "# training for 1 year with current parameters takes <10min\n",
    "\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 5\n",
    "RF_MAX_BINS = 10\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "# we should look into whether categoricalFeaturesInfo should be set to the categorical variables ?\n",
    "\n",
    "model = RandomForest.trainRegressor(training_data, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the model name if you dont want to overwrite a previous model ! (change date or version)\n",
    "%%time\n",
    "\n",
    "model_name = \"RF_test_model_26_6_2020\"\n",
    "model.save(sc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.000%\n",
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# getting the labels takes <10min\n",
    "\n",
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the labels_and_predictions RDD to a DataFrame\n",
    "\n",
    "def f(x):\n",
    "    d = {}\n",
    "    for i in range(len(x)):\n",
    "        d[str(i)] = x[i]\n",
    "    return d\n",
    "\n",
    "predictions_labels_df = labels_and_predictions.map(lambda x: Row(**f(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating...\n",
    "# for 2015 - rmse: 5.0768, mae: 2.8551\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"0\", predictionCol=\"1\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions_labels_df)\n",
    "print('RMSE: '+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.076857361810865"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_mae = RegressionEvaluator(labelCol=\"0\", predictionCol=\"1\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions_labels_df)\n",
    "print('MAE: '+str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8551452954946157"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
